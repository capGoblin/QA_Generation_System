{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRsp+oQCfLH979LWNb5LSb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/capGoblin/QA_Generation_System/blob/main/QA_Generation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "!pip install langchain\n",
        "!pip install PyPDF2\n",
        "!pip install pypdf\n",
        "!pip install tiktoken\n",
        "!pip install openai\n",
        "!pip install faiss-gpu"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9IBpZ_rROnhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QKhW94aDOR57"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import QAGenerationChain\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains import LLMChain\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "from PyPDF2 import PdfReader\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "lErchKeH2-wH",
        "outputId": "5ec731c3-59b2-47f5-efcb-5370706890d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f94af9d1-4d64-488b-b836-565f40f24f00\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f94af9d1-4d64-488b-b836-565f40f24f00\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Big Mac Index.pdf to Big Mac Index.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "Bg68AH7aeRpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_true_false_questions(text):\n",
        "    \"\"\"\n",
        "    Generate true/false questions based on the provided text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text content to base the questions on.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of generated true/false questions.\n",
        "    \"\"\"\n",
        "    # Initialize a language model for question generation\n",
        "    llm_ques_gen = ChatOpenAI(temperature=0.3, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Define the prompt template for true/false questions\n",
        "    true_false_prompt_template = \"\"\"\n",
        "    You are an expert at creating true/false questions based on the provided text.\n",
        "    Your goal is to test the knowledge of coders or programmers on the content below:\n",
        "\n",
        "    ------------\n",
        "    {text}\n",
        "    ------------\n",
        "\n",
        "    Create true/false questions that will assess understanding.\n",
        "    Ensure questions are clear and concise.\n",
        "\n",
        "    QUESTIONS:\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a prompt template with the specified input variable\n",
        "    prompt = PromptTemplate(input_variables=[\"text\"], template=true_false_prompt_template)\n",
        "\n",
        "    # Initialize an LLMChain for question generation\n",
        "    llmChain = LLMChain(llm=ChatOpenAI(temperature=0.3, model=\"gpt-3.5-turbo\"), prompt=prompt)\n",
        "\n",
        "    # Generate questions using the language model\n",
        "    questions = llmChain.run(text)\n",
        "\n",
        "    # Parse and format the generated questions\n",
        "    ques = []\n",
        "    questions = re.findall(r'(\\d+)\\.\\s+(.*)', questions)\n",
        "    for number, question in questions:\n",
        "        ques.append(f\"{number}. {question}\")\n",
        "\n",
        "    return ques\n",
        "\n",
        "def generate_multiple_choice_questions(text):\n",
        "    \"\"\"\n",
        "    Generate multiple-choice questions based on the provided text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text content to base the questions on.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of generated multiple-choice questions.\n",
        "    \"\"\"\n",
        "    # Define the prompt template for multiple-choice questions\n",
        "    prompt_template = \"\"\"\n",
        "    You are preparing multiple-choice questions based on the following text chunk:\n",
        "    ------------\n",
        "    {text}\n",
        "    ------------\n",
        "\n",
        "    Generate multiple-choice questions that cover important concepts.\n",
        "    Provide clear and relevant options.\n",
        "\n",
        "    QUESTIONS:\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a prompt template with the specified input variable\n",
        "    prompt = PromptTemplate(input_variables=[\"text\"], template=prompt_template)\n",
        "\n",
        "    # Initialize an LLMChain for question generation\n",
        "    llmChain = LLMChain(llm=ChatOpenAI(temperature=0.3, model=\"gpt-3.5-turbo\"), prompt=prompt)\n",
        "\n",
        "    # Generate questions using the language model\n",
        "    questions = llmChain.run(text)\n",
        "\n",
        "    # Split generated questions into blocks and format them\n",
        "    question_blocks = questions.strip().split('\\n\\n')\n",
        "    question_array = []\n",
        "    for block in question_blocks:\n",
        "        lines = block.strip().split('\\n')\n",
        "        question_number, question_content = lines[0].split('. ', 1)\n",
        "        options = ' '.join(f\"{line.strip()}\" for line in lines[1:])\n",
        "        formatted_question = f\"{question_number}. {question_content} {options}\"\n",
        "        question_array.append(formatted_question)\n",
        "\n",
        "    return question_array\n",
        "\n",
        "def generate_one_word_answer_questions(text):\n",
        "    \"\"\"\n",
        "    Generate one-word answer questions based on the provided text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text content to base the questions on.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of generated one-word answer questions.\n",
        "    \"\"\"\n",
        "    # Define the prompt template for one-word answer questions\n",
        "    prompt_template = \"\"\"\n",
        "    You are preparing one-word answer questions based on the following text chunk:\n",
        "    ------------\n",
        "    {text}\n",
        "    ------------\n",
        "\n",
        "    Generate one-word answer questions that target key information.\n",
        "    Keep questions precise and focused.\n",
        "\n",
        "    QUESTIONS:\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a prompt template with the specified input variable\n",
        "    prompt = PromptTemplate(input_variables=[\"text\"], template=prompt_template)\n",
        "\n",
        "    # Initialize an LLMChain for question generation\n",
        "    llmChain = LLMChain(llm=ChatOpenAI(temperature=0.3, model=\"gpt-3.5-turbo\"), prompt=prompt)\n",
        "\n",
        "    # Generate questions using the language model\n",
        "    questions = llmChain.run(text)\n",
        "\n",
        "    # Split generated questions into a list\n",
        "    questions = questions.split('\\n')\n",
        "\n",
        "    return questions\n"
      ],
      "metadata": {
        "id": "cEdS32yzIedP"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text_and_generate_questions(text):\n",
        "    splitter = TokenTextSplitter(model_name='gpt-3.5-turbo', chunk_size=10000, chunk_overlap=200)\n",
        "    text_chunks = splitter.split_text(text)\n",
        "\n",
        "    all_questions = {\n",
        "        \"true_false\": [],\n",
        "        \"multiple_choice\": [],\n",
        "        \"one_word_answer\": []\n",
        "    }\n",
        "\n",
        "    for chunk in text_chunks:\n",
        "        if chunk.strip():\n",
        "            # Generate questions for each chunk and append to respective lists\n",
        "            all_questions[\"true_false\"].extend(generate_true_false_questions(chunk))\n",
        "            all_questions[\"multiple_choice\"].extend(generate_multiple_choice_questions(chunk))\n",
        "            all_questions[\"one_word_answer\"].extend(generate_one_word_answer_questions(chunk))\n",
        "\n",
        "    return all_questions\n",
        "\n",
        "# Assuming correct implementation of PyPDFLoader and loader.load() to extract text\n",
        "loader = PyPDFLoader('/content/Big Mac Index.pdf')\n",
        "data = loader.load()\n",
        "\n",
        "question_gen = []\n",
        "\n",
        "for page in data:\n",
        "    question_gen.append(page.page_content)  # Accumulate page content in a list\n",
        "\n",
        "# Join all page contents into a single string\n",
        "generated_text = '\\n'.join(question_gen)\n",
        "\n",
        "# Process the concatenated text to generate questions\n",
        "generated_questions = process_text_and_generate_questions(generated_text)\n",
        "\n",
        "print(\"True/False Questions:\")\n",
        "print(generated_questions[\"true_false\"])\n",
        "\n",
        "print(\"\\nMultiple-Choice Questions:\")\n",
        "print(generated_questions[\"multiple_choice\"])\n",
        "\n",
        "print(\"\\nOne-Word Answer Questions:\")\n",
        "print(generated_questions[\"one_word_answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "beyKxmExIYI1",
        "outputId": "47a87665-cbc6-4210-9015-61415d8588f3"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True/False Questions:\n",
            "['1. The Big Mac Index was introduced in The Economist in September 1986 by Pam Woodall.', '2. The Big Mac Index compares the relative price worldwide to purchase a Whopper, a hamburger sold at Burger King restaurants.', '3. The purpose of the Big Mac Index is to calculate an implied exchange rate between two currencies.', \"4. The Big Mac Index is limited by geographical coverage due to the presence of the McDonald's franchise.\", '5. The Big Mac Index methodology is not affected by the social status of eating at fast food restaurants in a local market.']\n",
            "\n",
            "Multiple-Choice Questions:\n",
            "[\"1. What is the purpose of The Big Mac Index? A. To calculate the price of a Big Mac in different countries B. To measure purchasing power parity between two currencies C. To determine the nutritional value of a Big Mac D. To analyze the market share of McDonald's restaurants worldwide\", '2. How is the implied exchange rate calculated in The Big Mac Index? A. By dividing the price of a Big Mac in a foreign country by the price in a base country B. By comparing the prices of different fast-food items in various countries C. By converting the local prices of Starbucks coffee into USD D. By analyzing the GDP correlation with the price of a Big Mac', \"3. What limitation is mentioned in the text regarding The Big Mac Index? A. The index does not consider the social status of eating at fast-food restaurants B. The index is limited by geographical coverage due to the presence of McDonald's franchises C. The index does not account for the price differences of non-tradable goods and services D. The index is manipulated by the government to understate inflation rates\", '4. What is a criticism of using the Big Mac as a basis for the index? A. The Big Mac is not a popular item in most countries B. The nutritional values of the Big Mac vary from country to country C. The Big Mac is not exclusively beef in all countries D. The price of a Big Mac does not accurately reflect currency values', '5. Which country had one of the cheapest Big Macs in 2019 despite being an expensive city? A. Russia B. India C. Iceland D. Argentina']\n",
            "\n",
            "One-Word Answer Questions:\n",
            "['1. What is the purpose of the Big Mac Index?', '2. Who publishes the Big Mac Index?', '3. When was the Big Mac Index introduced?', '4. What does the Big Mac Index compare worldwide?', '5. How is the implied exchange rate calculated for the Big Mac Index?', '6. What is the theoretical basis for the Big Mac Index?', '7. What limitations does the Big Mac Index have in terms of geographical coverage?', '8. How does the Big Mac Index determine if a currency is undervalued or overvalued?', '9. What is the purpose of the variants of the Big Mac Index?', '10. What manipulation was reported in Argentina related to the Big Mac Index?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_true_false_answers(text, llm_model):\n",
        "    \"\"\"\n",
        "    Generate a true/false answer based on the provided text using a language model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text containing the statement to answer.\n",
        "        llm_model: The language model used for generating answers.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated true/false answer (either 'true' or 'false').\n",
        "    \"\"\"\n",
        "    # Define the prompt template for true/false answer generation\n",
        "    prompt_template = \"\"\"\n",
        "    You are an expert at answering true/false questions based on the provided text.\n",
        "    Please provide a true or false answer for the following statement:\n",
        "\n",
        "    ------------\n",
        "    {text}\n",
        "    ------------\n",
        "\n",
        "    ANSWER:\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a prompt template with the specified input variable\n",
        "    PROMPT_ANSWER = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
        "\n",
        "    # Initialize an LLMChain for answer generation\n",
        "    ques_gen_chain = LLMChain(llm=llm_model, prompt=PROMPT_ANSWER)\n",
        "\n",
        "    # Generate an answer using the language model\n",
        "    answer = ques_gen_chain.run(text=text)\n",
        "\n",
        "    # Clean up the generated answer (convert to lowercase and strip whitespace)\n",
        "    cleaned_answer = answer.strip().lower()\n",
        "\n",
        "    return cleaned_answer\n",
        "\n",
        "def generate_multiple_choice_answers(text, llm_model):\n",
        "    \"\"\"\n",
        "    Generate a multiple-choice answer based on the provided text using a language model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text containing the multiple-choice question.\n",
        "        llm_model: The language model used for generating answers.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated multiple-choice answer (e.g., 'A', 'B', 'C', etc.).\n",
        "    \"\"\"\n",
        "    # Define the prompt template for multiple-choice answer generation\n",
        "    prompt_template = \"\"\"\n",
        "    You are an expert at answering multiple-choice questions based on the provided text.\n",
        "    Please select the correct option (A, B, C, etc.) for the following question:\n",
        "\n",
        "    ------------\n",
        "    {text}\n",
        "    ------------\n",
        "\n",
        "    ANSWER:\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a prompt template with the specified input variable\n",
        "    PROMPT_ANSWER = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
        "\n",
        "    # Initialize an LLMChain for answer generation\n",
        "    ques_gen_chain = LLMChain(llm=llm_model, prompt=PROMPT_ANSWER)\n",
        "\n",
        "    # Generate an answer using the language model\n",
        "    answer = ques_gen_chain.run(text=text)\n",
        "\n",
        "    # Clean up the generated answer (convert to uppercase and strip whitespace)\n",
        "    cleaned_answer = answer.strip().upper()\n",
        "\n",
        "    return cleaned_answer\n",
        "\n",
        "def generate_one_word_answers(text, llm_model):\n",
        "    \"\"\"\n",
        "    Generate a one-word answer based on the provided text using a language model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text containing the question requiring a one-word answer.\n",
        "        llm_model: The language model used for generating answers.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated one-word answer.\n",
        "    \"\"\"\n",
        "    # Define the prompt template for one-word answer generation\n",
        "    prompt_template = \"\"\"\n",
        "    You are an expert at providing concise one-word answers based on the provided text.\n",
        "    Please provide a one-word answer to the following question:\n",
        "\n",
        "    ------------\n",
        "    {text}\n",
        "    ------------\n",
        "\n",
        "    ANSWER:\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a prompt template with the specified input variable\n",
        "    PROMPT_ANSWER = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
        "\n",
        "    # Initialize an LLMChain for answer generation\n",
        "    ques_gen_chain = LLMChain(llm=llm_model, prompt=PROMPT_ANSWER)\n",
        "\n",
        "    # Generate an answer using the language model\n",
        "    answer = ques_gen_chain.run(text=text)\n",
        "\n",
        "    # Clean up the generated answer (extract the first word and strip whitespace)\n",
        "    cleaned_answer = answer.strip().split()[0]\n",
        "\n",
        "    return cleaned_answer\n"
      ],
      "metadata": {
        "id": "D6K4km7iaIMv"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ans_for_generated_ques(generated_questions):\n",
        "    \"\"\"\n",
        "    Generate answers for the generated questions and write them to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        generated_questions (dict): A dictionary containing lists of generated questions.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Initialize a language model for generating answers\n",
        "    llm_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Dictionary to store question-answer mappings\n",
        "    question_answer_mapping = {}\n",
        "\n",
        "    # Generate answers for true/false questions\n",
        "    for question in generated_questions[\"true_false\"]:\n",
        "        generated_answer = generate_true_false_answers(question, llm_model)\n",
        "        question_answer_mapping[question] = generated_answer\n",
        "\n",
        "    # Add section header for true/false questions and answers\n",
        "    question_answer_mapping[\"=== TRUE/FALSE QUESTIONS ===\"] = \"=== ANSWERS BELOW ===\"\n",
        "\n",
        "    # Generate answers for multiple-choice questions\n",
        "    for question in generated_questions[\"multiple_choice\"]:\n",
        "        generated_answer = generate_multiple_choice_answers(question, llm_model)\n",
        "        question_answer_mapping[question] = generated_answer\n",
        "\n",
        "    # Add section header for multiple-choice questions and answers\n",
        "    question_answer_mapping[\"=== MULTIPLE CHOICE QUESTIONS ===\"] = \"=== ANSWERS BELOW ===\"\n",
        "\n",
        "    # Generate answers for one-word answer questions\n",
        "    for question in generated_questions[\"one_word_answer\"]:\n",
        "        generated_answer = generate_one_word_answers(question, llm_model)\n",
        "        question_answer_mapping[question] = generated_answer\n",
        "\n",
        "    # Add section header for one-word answer questions and answers\n",
        "    question_answer_mapping[\"=== ONE-WORD ANSWER QUESTIONS ===\"] = \"=== ANSWERS BELOW ===\"\n",
        "\n",
        "    # Write question-answer mappings to a CSV file\n",
        "    with open('question_answer_mapping.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['Question', 'Answer']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        # Write CSV header\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Write question-answer pairs to CSV rows\n",
        "        for question, answer in question_answer_mapping.items():\n",
        "            writer.writerow({'Question': question, 'Answer': answer})\n"
      ],
      "metadata": {
        "id": "EckOvroQVoLM"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_ans_for_generated_ques(generated_questions)"
      ],
      "metadata": {
        "id": "w48bniT8SlIv"
      },
      "execution_count": 126,
      "outputs": []
    }
  ]
}