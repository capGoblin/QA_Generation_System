{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "source": [
        "!pip install langchain\n",
        "!pip install PyPDF2\n",
        "!pip install pypdf\n",
        "!pip install tiktoken\n",
        "!pip install openai\n",
        "!pip install faiss-gpu"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9IBpZ_rROnhO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e0a5921d-6b47-41a6-a0e2-f64925316e6b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
            "  Downloading langchain_community-0.0.33-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
            "  Downloading langchain_core-0.1.44-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.49-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.33 langchain-core-0.1.44 langchain-text-splitters-0.0.1 langsmith-0.1.49 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.10.1 packaging-23.2 typing-inspect-0.9.0\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Installing collected packages: pypdf\n",
            "Successfully installed pypdf-4.2.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n",
            "Collecting openai\n",
            "  Downloading openai-1.23.1-py3-none-any.whl (310 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.0/311.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.23.1\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "QKhW94aDOR57"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import QAGenerationChain\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chains import LLMChain\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "from PyPDF2 import PdfReader\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "lErchKeH2-wH",
        "outputId": "637ce1a0-d5bb-4154-95c1-e7340c4e2c31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7c68afa8-079f-4d77-b964-6ef5100e50eb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7c68afa8-079f-4d77-b964-6ef5100e50eb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Big Mac Index.pdf to Big Mac Index.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "Bg68AH7aeRpA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_true_false_questions(text):\n",
        "    \"\"\"\n",
        "    Generate true/false questions based on the provided text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text content to base the questions on.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of generated true/false questions.\n",
        "    \"\"\"\n",
        "    # Initialize a language model for question generation\n",
        "    llm_ques_gen = ChatOpenAI(temperature=0.3, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Define the prompt template for true/false questions\n",
        "    true_false_prompt_template = \"\"\"\n",
        "    You are an expert at creating true/false questions based on the provided text.\n",
        "    Your goal is to test the knowledge of coders or programmers on the content below:\n",
        "\n",
        "    ------------\n",
        "    {text}\n",
        "    ------------\n",
        "\n",
        "    Create true/false questions that will assess understanding.\n",
        "    Ensure questions are clear and concise.\n",
        "\n",
        "    QUESTIONS:\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a prompt template with the specified input variable\n",
        "    prompt = PromptTemplate(input_variables=[\"text\"], template=true_false_prompt_template)\n",
        "\n",
        "    # Initialize an LLMChain for question generation\n",
        "    llmChain = LLMChain(llm=ChatOpenAI(temperature=0.3, model=\"gpt-3.5-turbo\"), prompt=prompt)\n",
        "\n",
        "    # Generate questions using the language model\n",
        "    questions = llmChain.run(text)\n",
        "\n",
        "    # Parse and format the generated questions\n",
        "    ques = []\n",
        "    questions = re.findall(r'(\\d+)\\.\\s+(.*)', questions)\n",
        "    for number, question in questions:\n",
        "        ques.append(f\"{number}. {question}\")\n",
        "\n",
        "    return ques\n",
        "\n",
        "def generate_multiple_choice_questions(text):\n",
        "    \"\"\"\n",
        "    Generate multiple-choice questions based on the provided text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text content to base the questions on.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of generated multiple-choice questions.\n",
        "    \"\"\"\n",
        "    # Define the prompt template for multiple-choice questions\n",
        "    prompt_template = \"\"\"\n",
        "    You are preparing multiple-choice questions based on the following text chunk:\n",
        "    ------------\n",
        "    {text}\n",
        "    ------------\n",
        "\n",
        "    Generate multiple-choice questions that cover important concepts.\n",
        "    Provide clear and relevant options.\n",
        "\n",
        "    QUESTIONS:\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a prompt template with the specified input variable\n",
        "    prompt = PromptTemplate(input_variables=[\"text\"], template=prompt_template)\n",
        "\n",
        "    # Initialize an LLMChain for question generation\n",
        "    llmChain = LLMChain(llm=ChatOpenAI(temperature=0.3, model=\"gpt-3.5-turbo\"), prompt=prompt)\n",
        "\n",
        "    # Generate questions using the language model\n",
        "    questions = llmChain.run(text)\n",
        "\n",
        "    # Split generated questions into blocks and format them\n",
        "    question_blocks = questions.strip().split('\\n\\n')\n",
        "    question_array = []\n",
        "    for block in question_blocks:\n",
        "        lines = block.strip().split('\\n')\n",
        "        question_number, question_content = lines[0].split('. ', 1)\n",
        "        options = ' '.join(f\"{line.strip()}\" for line in lines[1:])\n",
        "        formatted_question = f\"{question_number}. {question_content} {options}\"\n",
        "        question_array.append(formatted_question)\n",
        "\n",
        "    return question_array\n",
        "\n",
        "def generate_one_word_answer_questions(text):\n",
        "    \"\"\"\n",
        "    Generate one-word answer questions based on the provided text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text content to base the questions on.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of generated one-word answer questions.\n",
        "    \"\"\"\n",
        "    # Define the prompt template for one-word answer questions\n",
        "    prompt_template = \"\"\"\n",
        "    You are preparing one-word answer questions based on the following text chunk:\n",
        "    ------------\n",
        "    {text}\n",
        "    ------------\n",
        "\n",
        "    Generate one-word answer questions that target key information.\n",
        "    Keep questions precise and focused.\n",
        "\n",
        "    QUESTIONS:\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a prompt template with the specified input variable\n",
        "    prompt = PromptTemplate(input_variables=[\"text\"], template=prompt_template)\n",
        "\n",
        "    # Initialize an LLMChain for question generation\n",
        "    llmChain = LLMChain(llm=ChatOpenAI(temperature=0.3, model=\"gpt-3.5-turbo\"), prompt=prompt)\n",
        "\n",
        "    # Generate questions using the language model\n",
        "    questions = llmChain.run(text)\n",
        "\n",
        "    # Split generated questions into a list\n",
        "    questions = questions.split('\\n')\n",
        "\n",
        "    return questions\n"
      ],
      "metadata": {
        "id": "cEdS32yzIedP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text_and_generate_questions(text):\n",
        "    splitter = TokenTextSplitter(model_name='gpt-3.5-turbo', chunk_size=10000, chunk_overlap=200)\n",
        "    text_chunks = splitter.split_text(text)\n",
        "\n",
        "    all_questions = {\n",
        "        \"true_false\": [],\n",
        "        \"multiple_choice\": [],\n",
        "        \"one_word_answer\": []\n",
        "    }\n",
        "\n",
        "    for chunk in text_chunks:\n",
        "        if chunk.strip():\n",
        "            # Generate questions for each chunk and append to respective lists\n",
        "            all_questions[\"true_false\"] = generate_true_false_questions(chunk)\n",
        "            all_questions[\"multiple_choice\"] = generate_multiple_choice_questions(chunk)\n",
        "            all_questions[\"one_word_answer\"] = generate_one_word_answer_questions(chunk)\n",
        "\n",
        "    return all_questions\n",
        "\n",
        "# Assuming correct implementation of PyPDFLoader and loader.load() to extract text\n",
        "loader = PyPDFLoader('/content/Big Mac Index.pdf')\n",
        "data = loader.load()\n",
        "\n",
        "question_gen = []\n",
        "\n",
        "for page in data:\n",
        "    question_gen.append(page.page_content)  # Accumulate page content in a list\n",
        "\n",
        "# Join all page contents into a single string\n",
        "generated_text = '\\n'.join(question_gen)\n",
        "\n",
        "# Process the concatenated text to generate questions\n",
        "generated_questions = process_text_and_generate_questions(generated_text)\n",
        "\n",
        "print(\"True/False Questions:\")\n",
        "print(generated_questions[\"true_false\"])\n",
        "\n",
        "print(\"\\nMultiple-Choice Questions:\")\n",
        "print(generated_questions[\"multiple_choice\"])\n",
        "\n",
        "print(\"\\nOne-Word Answer Questions:\")\n",
        "print(generated_questions[\"one_word_answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "beyKxmExIYI1",
        "outputId": "67778e44-4b65-42b9-b537-bbf6241c44db"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True/False Questions:\n",
            "['1. The Big Mac Index was introduced in The Economist in September 1986 by Pam Woodall as a serious tool for exchange rate evaluation. (True/False)', '2. The Big Mac Index compares the relative price worldwide to purchase a Quarter Pounder with Cheese. (True/False)', '3. The Big Mac Index is based on the concept of Purchasing Power Parity (PPP). (True/False)', \"4. The Big Mac Index provides a method to analyze a currency's level of under/over-valuation against a base currency. (True/False)\", \"5. The Big Mac Index is limited by geographical coverage due to the presence of the McDonald's franchise. (True/False)\"]\n",
            "\n",
            "Multiple-Choice Questions:\n",
            "[\"1. What is the purpose of The Big Mac Index? A. To measure the popularity of McDonald's Big Mac sandwich B. To calculate an implied exchange rate between two currencies C. To determine the nutritional value of the Big Mac D. To analyze the cost of advertising for McDonald's\", '2. How is the implied exchange rate calculated in The Big Mac Index? A. By dividing the price of a Big Mac in a foreign country by the price in a base country B. By comparing the prices of different fast-food items worldwide C. By analyzing the cost of ingredients in a Big Mac D. By converting local prices of IKEA products into US dollars', \"3. What is one limitation of The Big Mac Index methodology? A. It only considers the prices of fast food items B. It does not take into account non-tradable goods and services C. It is limited by geographical coverage due to the presence of McDonald's franchises D. It does not provide an accurate measure of real-world purchasing power\", '4. How did the government in Argentina manipulate the Big Mac Index? A. By artificially lowering the price of Big Mac sandwiches B. By closing down McDonald\\'s restaurants in the country C. By falsifying consumer price data to understate inflation rates D. By introducing a new index called the \"KFC Index\"', '5. What is a variant of The Big Mac Index mentioned in the text? A. The Billy index comparing prices of IKEA products B. The Latte Line measuring the correlation between coffee prices and GDP C. The Chai Latte Global Index comparing Starbucks Chai Latte prices D. The Gold-Mac-Index calculating the purchasing power of gold']\n",
            "\n",
            "One-Word Answer Questions:\n",
            "['1. What is the purpose of the Big Mac Index?', '2. Who introduced the Big Mac Index in The Economist in September 1986?', \"3. How is the Big Mac Index used to analyze a currency's level of valuation?\", '4. What limitations does the Big Mac Index face in terms of geographical coverage?', '5. How did the government of Argentina manipulate the price of Big Macs to influence the Big Mac Index?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def file_processing(file_path):\n",
        "\n",
        "    # Load data from PDF\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    data = loader.load()\n",
        "\n",
        "    question_gen = ''\n",
        "\n",
        "    for page in data:\n",
        "        question_gen += page.page_content\n",
        "\n",
        "    splitter_ques_gen = TokenTextSplitter(\n",
        "        model_name = 'gpt-3.5-turbo',\n",
        "        chunk_size = 10000,\n",
        "        chunk_overlap = 200\n",
        "    )\n",
        "\n",
        "    chunks_ques_gen = splitter_ques_gen.split_text(question_gen)\n",
        "\n",
        "    document_ques_gen = [Document(page_content=t) for t in chunks_ques_gen]\n",
        "\n",
        "    splitter_ans_gen = TokenTextSplitter(\n",
        "        model_name = 'gpt-3.5-turbo',\n",
        "        chunk_size = 1000,\n",
        "        chunk_overlap = 100\n",
        "    )\n",
        "\n",
        "\n",
        "    document_answer_gen = splitter_ans_gen.split_documents(\n",
        "        document_ques_gen\n",
        "    )\n",
        "\n",
        "    return document_ques_gen, document_answer_gen"
      ],
      "metadata": {
        "id": "VdMv9qZ17ENn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_true_false_answers(text, answer_generation_chain):\n",
        "    \"\"\"\n",
        "    Generate a true/false answer based on the provided text using a language model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text containing the statement to answer.\n",
        "        llm_model: The language model used for generating answers.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated true/false answer (either 'true' or 'false').\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    result = answer_generation_chain({\"query\": text + 'Only Answer.'})\n",
        "\n",
        "    # Clean up the generated answer (convert to lowercase and strip whitespace)\n",
        "    cleaned_answer = result['result'].strip().lower()\n",
        "\n",
        "    return cleaned_answer\n",
        "\n",
        "def generate_multiple_choice_answers(text, answer_generation_chain):\n",
        "    \"\"\"\n",
        "    Generate a multiple-choice answer based on the provided text using a language model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text containing the multiple-choice question.\n",
        "        llm_model: The language model used for generating answers.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated multiple-choice answer (e.g., 'A', 'B', 'C', etc.).\n",
        "    \"\"\"\n",
        "\n",
        "    result = answer_generation_chain({\"query\": text + 'Please select the correct option (A, B, C, etc.) for the following question. Only Answer.'})\n",
        "\n",
        "    cleaned_answer = result['result'].strip().upper()\n",
        "\n",
        "    return cleaned_answer\n",
        "\n",
        "def generate_one_word_answers(text, answer_generation_chain):\n",
        "    \"\"\"\n",
        "    Generate a one-word answer based on the provided text using a language model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text containing the question requiring a one-word answer.\n",
        "        llm_model: The language model used for generating answers.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated one-word answer.\n",
        "    \"\"\"\n",
        "\n",
        "    result = answer_generation_chain({\"query\": text + 'Please provide a one-word answer to the following question. Only Answer.'})\n",
        "\n",
        "\n",
        "    # Clean up the generated answer (convert to uppercase and strip whitespace)\n",
        "    cleaned_answer = result['result'].strip().lower()\n",
        "\n",
        "    return cleaned_answer\n"
      ],
      "metadata": {
        "id": "D6K4km7iaIMv"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ans_for_generated_ques(generated_questions):\n",
        "    \"\"\"\n",
        "    Generate answers for the generated questions and write them to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        generated_questions (dict): A dictionary containing lists of generated questions.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    document_ques_gen, document_answer_gen = file_processing('/content/Big Mac Index.pdf')\n",
        "\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "\n",
        "    vector_store = FAISS.from_documents(document_answer_gen, embeddings)\n",
        "\n",
        "    llm_answer_gen = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "    answer_generation_chain = RetrievalQA.from_chain_type(llm=llm_answer_gen,\n",
        "                                                chain_type=\"stuff\",\n",
        "                                                retriever=vector_store.as_retriever())\n",
        "\n",
        "\n",
        "    # Dictionary to store question-answer mappings\n",
        "    question_answer_mapping = {}\n",
        "\n",
        "    # Generate answers for true/false questions\n",
        "    # Add section header for true/false questions and answers\n",
        "    question_answer_mapping[\"=== TRUE/FALSE QUESTIONS ===\"] = \"=== ANSWERS BELOW ===\"\n",
        "    for question in generated_questions[\"true_false\"]:\n",
        "        generated_answer = generate_true_false_answers(question, answer_generation_chain)\n",
        "        question_answer_mapping[question] = generated_answer\n",
        "    # print(question_answer_mapping)\n",
        "\n",
        "\n",
        "    # # Add section header for multiple-choice questions and answers\n",
        "    question_answer_mapping[\"=== MULTIPLE CHOICE QUESTIONS ===\"] = \"=== ANSWERS BELOW ===\"\n",
        "    # Generate answers for multiple-choice questions\n",
        "    for question in generated_questions[\"multiple_choice\"]:\n",
        "        generated_answer = generate_multiple_choice_answers(question, answer_generation_chain)\n",
        "        question_answer_mapping[question] = generated_answer\n",
        "\n",
        "\n",
        "    # Add section header for one-word answer questions and answers\n",
        "    question_answer_mapping[\"=== ONE-WORD ANSWER QUESTIONS ===\"] = \"=== ANSWERS BELOW ===\"\n",
        "    # Generate answers for one-word answer questions\n",
        "    for question in generated_questions[\"one_word_answer\"]:\n",
        "        generated_answer = generate_one_word_answers(question, answer_generation_chain)\n",
        "        question_answer_mapping[question] = generated_answer\n",
        "\n",
        "    print(question_answer_mapping)\n",
        "\n",
        "\n",
        "    # # Write question-answer mappings to a CSV file\n",
        "    with open('question_answer_mapping.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['Question', 'Answer']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        # Write CSV header\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Write question-answer pairs to CSV rows\n",
        "        for question, answer in question_answer_mapping.items():\n",
        "            writer.writerow({'Question': question, 'Answer': answer})\n"
      ],
      "metadata": {
        "id": "EckOvroQVoLM"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_ans_for_generated_ques(generated_questions)"
      ],
      "metadata": {
        "id": "w48bniT8SlIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e24f60e3-83c4-4b95-9e6e-4fa3edf576c4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'=== TRUE/FALSE QUESTIONS ===': '=== ANSWERS BELOW ===', '1. The Big Mac Index was introduced in The Economist in September 1986 by Pam Woodall as a serious tool for exchange rate evaluation. (True/False)': 'false', '2. The Big Mac Index compares the relative price worldwide to purchase a Quarter Pounder with Cheese. (True/False)': 'false', '3. The Big Mac Index is based on the concept of Purchasing Power Parity (PPP). (True/False)': 'true', \"4. The Big Mac Index provides a method to analyze a currency's level of under/over-valuation against a base currency. (True/False)\": 'true', \"5. The Big Mac Index is limited by geographical coverage due to the presence of the McDonald's franchise. (True/False)\": 'true', '=== MULTIPLE CHOICE QUESTIONS ===': '=== ANSWERS BELOW ===', \"1. What is the purpose of The Big Mac Index? A. To measure the popularity of McDonald's Big Mac sandwich B. To calculate an implied exchange rate between two currencies C. To determine the nutritional value of the Big Mac D. To analyze the cost of advertising for McDonald's\": 'B. TO CALCULATE AN IMPLIED EXCHANGE RATE BETWEEN TWO CURRENCIES', '2. How is the implied exchange rate calculated in The Big Mac Index? A. By dividing the price of a Big Mac in a foreign country by the price in a base country B. By comparing the prices of different fast-food items worldwide C. By analyzing the cost of ingredients in a Big Mac D. By converting local prices of IKEA products into US dollars': 'A. BY DIVIDING THE PRICE OF A BIG MAC IN A FOREIGN COUNTRY BY THE PRICE IN A BASE COUNTRY', \"3. What is one limitation of The Big Mac Index methodology? A. It only considers the prices of fast food items B. It does not take into account non-tradable goods and services C. It is limited by geographical coverage due to the presence of McDonald's franchises D. It does not provide an accurate measure of real-world purchasing power\": 'B. IT DOES NOT TAKE INTO ACCOUNT NON-TRADABLE GOODS AND SERVICES', '4. How did the government in Argentina manipulate the Big Mac Index? A. By artificially lowering the price of Big Mac sandwiches B. By closing down McDonald\\'s restaurants in the country C. By falsifying consumer price data to understate inflation rates D. By introducing a new index called the \"KFC Index\"': 'A. BY ARTIFICIALLY LOWERING THE PRICE OF BIG MAC SANDWICHES', '5. What is a variant of The Big Mac Index mentioned in the text? A. The Billy index comparing prices of IKEA products B. The Latte Line measuring the correlation between coffee prices and GDP C. The Chai Latte Global Index comparing Starbucks Chai Latte prices D. The Gold-Mac-Index calculating the purchasing power of gold': 'C. THE CHAI LATTE GLOBAL INDEX COMPARING STARBUCKS CHAI LATTE PRICES', '=== ONE-WORD ANSWER QUESTIONS ===': '=== ANSWERS BELOW ===', '1. What is the purpose of the Big Mac Index?': 'COMPARISON', '2. Who introduced the Big Mac Index in The Economist in September 1986?': 'PAM', \"3. How is the Big Mac Index used to analyze a currency's level of valuation?\": 'COMPARISON', '4. What limitations does the Big Mac Index face in terms of geographical coverage?': 'GEOGRAPHICAL', '5. How did the government of Argentina manipulate the price of Big Macs to influence the Big Mac Index?': 'MANIPULATION'}\n"
          ]
        }
      ]
    }
  ]
}